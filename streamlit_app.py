# streamlit_app.py
# ëŒ€ê¸°ì§ˆ ê´€ì¸¡ì†Œ ëŒ€ì‹œë³´ë“œ: [í˜„í™©] [ì˜ˆì¸¡] [ì•ŒëžŒ] íŽ˜ì´ì§€ ë¶„ë¦¬ + ì¢…í•© AQI í‘œì‹œ ë³´ê°•
# ì‹¤í–‰: streamlit run streamlit_app.py

from __future__ import annotations

import json
import os
import urllib.request
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import streamlit as st
import streamlit.components.v1 as components
from streamlit.errors import StreamlitSecretNotFoundError


# =========================================================
# (ì˜µì…˜) Plotly / Matplotlib (ì—†ìœ¼ë©´ st.line_chart)
# =========================================================
PLOTLY_OK = True
try:
    import plotly.graph_objects as go  # noqa: F401
except ModuleNotFoundError:
    PLOTLY_OK = False

MPL_OK = True
try:
    import matplotlib.pyplot as plt  # noqa: F401
except ModuleNotFoundError:
    MPL_OK = False

# =========================================================
# (ì˜µì…˜) scikit-learn (ì—†ìœ¼ë©´ numpy Ridgeë¡œ ëŒ€ì²´)
# =========================================================
SKLEARN_OK = True
try:
    from sklearn.ensemble import HistGradientBoostingRegressor
    from sklearn.metrics import mean_absolute_error
except ModuleNotFoundError:
    SKLEARN_OK = False


# =========================================================
# Streamlit ê¸°ë³¸ ì„¤ì •
# =========================================================
st.set_page_config(page_title="ëŒ€ê¸°ì§ˆ ê´€ì¸¡ì†Œ ëŒ€ì‹œë³´ë“œ", layout="wide")
st.title("ëŒ€ê¸°ì§ˆ ê´€ì¸¡ì†Œ ëŒ€ì‹œë³´ë“œ")


# =========================================================
# ì „ì—­ ìƒìˆ˜
# =========================================================
DEFAULT_CANDIDATES = [
    "pollution_2018_2023_3.csv",
    "./data/pollution_2018_2023_3.csv",
    "/mnt/data/pollution_2018_2023_3.csv",
]

POLLUTANTS = ["o3", "no2", "co", "so2"]
AQI_COLS = [f"{p}_aqi" for p in POLLUTANTS]
MEAN_COLS = [f"{p}_mean" for p in POLLUTANTS]
MET_COLS = ["temp_c", "pressure_pa", "met_rain_mm", "met_wind_u", "met_wind_v"]

AQI_BANDS = [
    (0, 50, "ì¢‹ìŒ(Good)"),
    (51, 100, "ë³´í†µ(Moderate)"),
    (101, 150, "ë¯¼ê°êµ° ë‚˜ì¨(USG)"),
    (151, 200, "ë‚˜ì¨(Unhealthy)"),
    (201, 300, "ë§¤ìš° ë‚˜ì¨(Very Unhealthy)"),
    (301, 500, "ìœ„í—˜(Hazardous)"),
]


# =========================================================
# Secrets ì•ˆì „ ì ‘ê·¼ (secrets.toml ì—†ì–´ë„ ì•± ì‹¤í–‰)
# =========================================================
def get_secret_safe(key: str, default: str = "") -> str:
    try:
        return st.secrets.get(key, default)
    except StreamlitSecretNotFoundError:
        return os.environ.get(key, default)
    except Exception:
        return os.environ.get(key, default)


# =========================================================
# ìœ í‹¸
# =========================================================
def aqi_category(v: float) -> str:
    if pd.isna(v):
        return "N/A"
    v = float(v)
    for lo, hi, name in AQI_BANDS:
        if lo <= v <= hi:
            return name
    if v < 0:
        return "N/A"
    return "ìœ„í—˜(Hazardous)"


def safe_float(x) -> Optional[float]:
    try:
        return float(x)
    except Exception:
        return None


def parse_geometry_point(geo_str: str) -> Tuple[Optional[float], Optional[float]]:
    """
    geometry ì»¬ëŸ¼: GeoJSON ë¬¸ìžì—´ ê°€ì •
      {"type":"Point","coordinates":[lon, lat]}
    """
    if not isinstance(geo_str, str) or not geo_str.strip():
        return None, None
    try:
        obj = json.loads(geo_str)
        coords = obj.get("coordinates", None)
        if isinstance(coords, list) and len(coords) >= 2:
            lon = safe_float(coords[0])
            lat = safe_float(coords[1])
            return lat, lon
    except Exception:
        return None, None
    return None, None


def candidate_default_path() -> Optional[str]:
    for p in DEFAULT_CANDIDATES:
        if os.path.exists(p):
            return p
    return None


def toast(msg: str, icon: str = "â„¹ï¸"):
    if hasattr(st, "toast"):
        st.toast(msg, icon=icon)


def compute_overall_aqi_row(row: pd.Series) -> float:
    """í–‰ ë‹¨ìœ„ ì¢…í•© AQI(4ê°œ ì˜¤ì—¼ë¬¼ì§ˆ AQI ìµœëŒ€ê°’) - ê¸°ì¡´ overall_aqiê°€ NaNì¼ ë•Œ ë³´ê°•"""
    vals = []
    for c in AQI_COLS:
        v = row.get(c, np.nan)
        if pd.notna(v):
            vals.append(float(v))
    return float(np.nanmax(vals)) if vals else np.nan


def pick_latest_valid_row(df_site: pd.DataFrame, prefer_cols: List[str]) -> pd.Series:
    """
    ìµœì‹  í–‰ì´ ì „ì²´ NaNì¸ ê²½ìš°ê°€ ìžˆì–´, 'ì¢…í•©AQI/ì˜¤ì—¼ë¬¼ì§ˆAQI ì¤‘ í•˜ë‚˜ë¼ë„ ìœ íš¨'í•œ ìµœì‹  í–‰ì„ ì„ íƒ.
    """
    d = df_site.sort_values("date")
    mask = np.zeros(len(d), dtype=bool)
    for c in prefer_cols:
        if c in d.columns:
            mask |= d[c].notna().to_numpy()
    if mask.any():
        return d.loc[mask].iloc[-1]
    return d.iloc[-1]


# =========================================================
# ì°¨íŠ¸ ë Œë”ëŸ¬(Plotly â†’ Matplotlib â†’ st.line_chart)
# =========================================================
def render_multi_line(df: pd.DataFrame, x_col: str, y_cols: List[str], title: str, y_label: str, height: int = 420):
    use_cols = [x_col] + [c for c in y_cols if c in df.columns]
    dfp = df[use_cols].copy().dropna(subset=[x_col])
    if len(dfp) == 0:
        st.info("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    if PLOTLY_OK:
        fig = go.Figure()
        for c in y_cols:
            if c in dfp.columns:
                fig.add_trace(go.Scatter(x=dfp[x_col], y=dfp[c], mode="lines", name=c))
        fig.update_layout(
            height=height,
            margin=dict(l=10, r=10, t=40, b=10),
            legend=dict(orientation="h"),
            xaxis_title=x_col,
            yaxis_title=y_label,
            title=title,
        )
        st.plotly_chart(fig, use_container_width=True)
        return

    if MPL_OK:
        import matplotlib.pyplot as plt  # local import

        fig, ax = plt.subplots()
        for c in y_cols:
            if c in dfp.columns:
                ax.plot(dfp[x_col], dfp[c], label=c)
        ax.set_title(title)
        ax.set_xlabel(x_col)
        ax.set_ylabel(y_label)
        ax.legend()
        st.pyplot(fig, clear_figure=True)
        return

    st.line_chart(dfp.set_index(x_col)[[c for c in y_cols if c in dfp.columns]], height=height)


def render_single_line(df: pd.DataFrame, x_col: str, y_col: str, title: str, y_label: str, height: int = 240):
    render_multi_line(df, x_col, [y_col], title, y_label, height)


# =========================================================
# ë°ì´í„° ë¡œë“œ/ì •ê·œí™”
# =========================================================
@st.cache_data(show_spinner=False)
def load_csv(path: str) -> pd.DataFrame:
    return pd.read_csv(path)


@st.cache_data(show_spinner=False)
def normalize_data(df_raw: pd.DataFrame) -> pd.DataFrame:
    df = df_raw.copy()

    required = {"site", "city", "county", "state", "date", "geometry"} | set(AQI_COLS) | set(MEAN_COLS)
    missing = sorted(list(required - set(df.columns)))
    if missing:
        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")

    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df = df.dropna(subset=["date"]).copy()

    for c in AQI_COLS + MEAN_COLS + [c for c in MET_COLS if c in df.columns]:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    if "is_observed" in df.columns:
        df["is_observed"] = pd.to_numeric(df["is_observed"], errors="coerce").fillna(0).astype(int)
    else:
        df["is_observed"] = 1

    if "is_imputed" in df.columns:
        df["is_imputed"] = pd.to_numeric(df["is_imputed"], errors="coerce").fillna(0).astype(int)
    else:
        df["is_imputed"] = 0

    latlon = df["geometry"].apply(parse_geometry_point)
    df["lat"] = latlon.apply(lambda t: t[0])
    df["lon"] = latlon.apply(lambda t: t[1])

    # ì¢…í•© AQI(ë³´ìˆ˜ì  ìš´ì˜): 4ê°œ AQIì˜ ìµœëŒ€ê°’
    df["overall_aqi"] = df[AQI_COLS].max(axis=1, skipna=True)

    # ì¢…í•© AQIê°€ NaNìœ¼ë¡œ ë‚¨ëŠ” ì¼€ì´ìŠ¤ ë³´ê°•(í–‰ ë‹¨ìœ„ ìž¬ê³„ì‚°)
    # (ì˜ˆ: ì¼ë¶€ ì»¬ëŸ¼ì´ objectë¡œ ë‚¨ì•˜ë‹¤ê°€ numeric ë³€í™˜ ì‹¤íŒ¨í•œ ê²½ìš°, ë˜ëŠ” íŠ¹ì • í–‰ AQI ëª¨ë‘ NaNì¸ ê²½ìš°)
    # -> numeric ë³€í™˜ì€ í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” "all NaN" í–‰ì„ ê·¸ëŒ€ë¡œ ë‘ë˜ KPI ì„ íƒì—ì„œ ìœ íš¨ê°’ í–‰ì„ ìš°ì„  ì„ íƒí•˜ë„ë¡ ì²˜ë¦¬í•¨.

    def _main_pollutant(row) -> str:
        vals = {p: row.get(f"{p}_aqi", np.nan) for p in POLLUTANTS}
        vals = {k: v for k, v in vals.items() if pd.notna(v)}
        if not vals:
            return "N/A"
        return max(vals, key=vals.get).upper()

    df["main_pollutant"] = df.apply(_main_pollutant, axis=1)
    df["overall_cat"] = df["overall_aqi"].apply(aqi_category)

    df = df.sort_values(["site", "date"]).reset_index(drop=True)
    return df


# =========================================================
# ì˜ˆì¸¡(ì˜µì…˜): sklearn ìžˆìœ¼ë©´ HGBR, ì—†ìœ¼ë©´ Ridge(ì„ í˜•) fallback
# =========================================================
def make_time_features(dts: pd.Series) -> pd.DataFrame:
    d = pd.to_datetime(dts)
    out = pd.DataFrame(index=d.index)
    out["dow"] = d.dt.dayofweek.astype(int)
    out["month"] = d.dt.month.astype(int)
    out["doy"] = d.dt.dayofyear.astype(int)
    out["doy_sin"] = np.sin(2 * np.pi * out["doy"] / 365.25)
    out["doy_cos"] = np.cos(2 * np.pi * out["doy"] / 365.25)
    return out


def make_supervised(ts: pd.Series, dates: pd.Series, lags: int = 14, roll_windows: List[int] = [3, 7, 14]) -> pd.DataFrame:
    df = pd.DataFrame({"y": ts.values}, index=pd.to_datetime(dates))
    for k in range(1, lags + 1):
        df[f"lag_{k}"] = df["y"].shift(k)
    for w in roll_windows:
        df[f"roll_mean_{w}"] = df["y"].shift(1).rolling(w).mean()
        df[f"roll_std_{w}"] = df["y"].shift(1).rolling(w).std(ddof=0)

    tf = make_time_features(df.index.to_series())
    X = pd.concat([df.drop(columns=["y"]), tf], axis=1)
    y = df["y"]
    out = pd.concat([X, y], axis=1).dropna()
    return out


def mean_absolute_error_np(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    return float(np.nanmean(np.abs(y_true - y_pred)))


def ridge_fit(A: np.ndarray, b: np.ndarray, alpha: float = 2.0) -> np.ndarray:
    I = np.eye(A.shape[1])
    I[0, 0] = 0.0
    return np.linalg.solve(A.T @ A + alpha * I, A.T @ b)


def ridge_predict(A: np.ndarray, w: np.ndarray) -> np.ndarray:
    return A @ w


@dataclass
class ForecastResult:
    pred_df: pd.DataFrame
    mae: Optional[float]


@st.cache_data(show_spinner=False)
def train_and_forecast_site(df_site: pd.DataFrame, target_col: str, horizon: int, lags: int = 14) -> ForecastResult:
    d = df_site[["date", target_col]].dropna().sort_values("date").copy()
    if len(d) < (lags + 60):
        return ForecastResult(pred_df=pd.DataFrame(columns=["date", "pred"]), mae=None)

    sup = make_supervised(d[target_col], d["date"], lags=lags)
    X = sup.drop(columns=["y"])
    y = sup["y"]

    test_n = min(90, max(30, int(len(sup) * 0.15)))
    X_train, y_train = X.iloc[:-test_n], y.iloc[:-test_n]
    X_test, y_test = X.iloc[-test_n:], y.iloc[-test_n:]

    model = None
    mae = None

    if SKLEARN_OK:
        model = HistGradientBoostingRegressor(max_depth=6, learning_rate=0.08, max_iter=400, random_state=42)
        model.fit(X_train, y_train)
        y_hat = model.predict(X_test)
        mae = float(mean_absolute_error(y_test, y_hat))
        w = None
        cols = X_train.columns.tolist()
    else:
        cols = X_train.columns.tolist()
        Xt = X_train.replace([np.inf, -np.inf], np.nan).dropna()
        yt = y_train.loc[Xt.index].astype(float)

        A = Xt.to_numpy(dtype=float)
        b = yt.to_numpy(dtype=float)
        A = np.c_[np.ones(len(A)), A]
        w = ridge_fit(A, b, alpha=2.0)

        Xv = X_test.replace([np.inf, -np.inf], np.nan).ffill().bfill().fillna(0.0)
        Av = Xv.to_numpy(dtype=float)
        Av = np.c_[np.ones(len(Av)), Av]
        y_hat = ridge_predict(Av, w)
        mae = mean_absolute_error_np(y_test.to_numpy(dtype=float), y_hat)

    history = d.set_index("date")[target_col].copy()
    last_date = history.index.max()
    future_dates = pd.date_range(last_date + pd.Timedelta(days=1), periods=horizon, freq="D")

    preds: List[float] = []
    hist_vals = history.copy()

    def build_row(dt: pd.Timestamp) -> pd.DataFrame:
        row = {}
        for k in range(1, lags + 1):
            row[f"lag_{k}"] = float(hist_vals.iloc[-k]) if len(hist_vals) >= k else np.nan
        for wdw in [3, 7, 14]:
            if len(hist_vals) >= wdw:
                row[f"roll_mean_{wdw}"] = float(hist_vals.iloc[-wdw:].mean())
                row[f"roll_std_{wdw}"] = float(hist_vals.iloc[-wdw:].std(ddof=0))
            else:
                row[f"roll_mean_{wdw}"] = np.nan
                row[f"roll_std_{wdw}"] = np.nan

        tf = make_time_features(pd.Series([dt]))
        for c in tf.columns:
            row[c] = float(tf.iloc[0][c])

        x_row = pd.DataFrame([row])
        for c in cols:
            if c not in x_row.columns:
                x_row[c] = np.nan
        return x_row[cols]

    for dt in future_dates:
        x_row = build_row(dt)
        if x_row.isna().any(axis=1).iloc[0]:
            pred = float(hist_vals.iloc[-1])
        else:
            if SKLEARN_OK and model is not None:
                pred = float(model.predict(x_row)[0])
            else:
                Xp = x_row.replace([np.inf, -np.inf], np.nan).ffill().bfill().fillna(0.0)
                Ap = Xp.to_numpy(dtype=float)
                Ap = np.c_[np.ones(len(Ap)), Ap]
                pred = float(ridge_predict(Ap, w)[0])
        preds.append(pred)
        hist_vals.loc[dt] = pred

    return ForecastResult(pred_df=pd.DataFrame({"date": future_dates, "pred": preds}), mae=mae)


def make_climatology(df_site: pd.DataFrame, target_col: str) -> pd.Series:
    d = df_site[["date", target_col]].dropna().copy()
    d["doy"] = d["date"].dt.dayofyear
    return d.groupby("doy")[target_col].mean()


def sustained_flags(values: pd.Series, threshold: float) -> pd.Series:
    cnt = 0
    out = []
    for v in values:
        if pd.notna(v) and v >= threshold:
            cnt += 1
        else:
            cnt = 0
        out.append(cnt)
    return pd.Series(out, index=values.index)


# =========================================================
# ì•ŒëžŒ í‰ê°€ + Slack Webhook(ì„ íƒ)
# =========================================================
def send_slack_webhook(webhook_url: str, text: str) -> bool:
    if not webhook_url:
        return False
    payload = json.dumps({"text": text}).encode("utf-8")
    req = urllib.request.Request(
        webhook_url,
        data=payload,
        headers={"Content-Type": "application/json"},
        method="POST",
    )
    try:
        with urllib.request.urlopen(req, timeout=10) as resp:
            return 200 <= resp.status < 300
    except Exception:
        return False


def evaluate_alert_state(
    df_site: pd.DataFrame,
    target_col: str,
    alert_threshold: float,
    sustain_days: int,
    anom_threshold: float,
    delta_threshold: float,
    earlywarn_days: int,
    forecast_df: Optional[pd.DataFrame] = None,  # columns: date, pred
) -> Dict[str, object]:
    reasons: List[str] = []
    level = "NORMAL"
    is_alert = False

    d = df_site[["date", target_col]].dropna().sort_values("date").copy()
    if len(d) < max(30, sustain_days + 5):
        return {"level": "NORMAL", "reasons": ["ë°ì´í„° ë¶€ì¡±"], "is_alert": False}

    # 1) ìž„ê³„ ì´ˆê³¼ + ì§€ì†
    d_tail = d.tail(max(30, sustain_days + 10)).copy()
    d_tail["sustain"] = sustained_flags(d_tail[target_col], float(alert_threshold)).astype(int)
    if int(d_tail["sustain"].iloc[-1]) >= int(sustain_days):
        is_alert = True
        reasons.append(f"ìž„ê³„ê°’({alert_threshold:.0f}) ì´ˆê³¼ {sustain_days}ì¼ ì§€ì†")

    # 2) anomaly(í´ë¼ì´ë§ˆí†¨ë¡œì§€ ëŒ€ë¹„)
    clim = make_climatology(df_site, target_col)
    last = d.iloc[-1]
    doy = int(pd.to_datetime(last["date"]).dayofyear)
    clim_v = float(clim.get(doy, np.nan))
    if pd.notna(clim_v) and pd.notna(last[target_col]):
        anom = float(last[target_col] - clim_v)
        if anom >= float(anom_threshold):
            reasons.append(f"anomaly +{anom:.1f} (ê¸°ì¤€ +{anom_threshold:.0f})")
            level = "WATCH"

    # 3) ì „ì¼ ëŒ€ë¹„ ê¸‰ë“±(Î”)
    if len(d) >= 2:
        delta = float(d.iloc[-1][target_col] - d.iloc[-2][target_col])
        if delta >= float(delta_threshold):
            reasons.append(f"ì „ì¼ ëŒ€ë¹„ +{delta:.1f} (ê¸°ì¤€ +{delta_threshold:.0f})")
            level = "WATCH"

    # 4) ì¡°ê¸°ê²½ë³´(ì˜ˆì¸¡ ê¸°ë°˜)
    if forecast_df is not None and not forecast_df.empty and earlywarn_days > 0:
        f = forecast_df.sort_values("date").head(int(earlywarn_days))
        if (f["pred"] >= float(alert_threshold)).any():
            reasons.append(f"ì¡°ê¸°ê²½ë³´: {earlywarn_days}ì¼ ì´ë‚´ ìž„ê³„ ì´ˆê³¼ ì˜ˆì¸¡")
            if level == "NORMAL":
                level = "WATCH"

    if is_alert:
        level = "ALERT"

    return {"level": level, "reasons": reasons, "is_alert": is_alert}


# =========================================================
# ì‚¬ì´ë“œë°”: ê³µí†µ(ë°ì´í„°/í•„í„°/íŽ˜ì´ì§€)
# =========================================================
with st.sidebar:
    st.header("íŽ˜ì´ì§€")
    page = st.radio("ì´ë™", ["í˜„í™©", "ì˜ˆì¸¡", "ì•ŒëžŒ"], index=0)

    st.divider()
    st.header("ë°ì´í„°")
    default_path = candidate_default_path()
    uploaded = st.file_uploader("CSV ì—…ë¡œë“œ(ì˜µì…˜)", type=["csv"])
    if uploaded is None:
        st.caption("ì—…ë¡œë“œê°€ ì—†ìœ¼ë©´ ê²½ë¡œ ìž…ë ¥/ê¸°ë³¸ ê²½ë¡œì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.")
        st.text_input("CSV ê²½ë¡œ", value=default_path or "", key="csv_path")
    else:
        st.session_state["csv_path"] = ""

    st.divider()
    st.header("ëª¨ë‹ˆí„°ë§")
    refresh_sec = st.number_input("ìžë™ ê°±ì‹ (ì´ˆ) - 0ì´ë©´ OFF", min_value=0, max_value=3600, value=0, step=10)
    if refresh_sec and refresh_sec > 0:
        components.html(f"<meta http-equiv='refresh' content='{int(refresh_sec)}'>", height=0)

# ë°ì´í„° ë¡œë“œ
try:
    if uploaded is not None:
        raw = pd.read_csv(uploaded)
    else:
        csv_path = (st.session_state.get("csv_path") or "").strip()
        if not csv_path:
            if default_path is None:
                st.error("CSVë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ ìž…ë ¥í•´ ì£¼ì„¸ìš”.")
                st.stop()
            csv_path = default_path
        raw = load_csv(csv_path)

    df = normalize_data(raw)
except Exception as e:
    st.error(f"ë°ì´í„° ë¡œë“œ/ì •ê·œí™” ì˜¤ë¥˜: {e}")
    st.stop()

site_counts = df.groupby("site").size().sort_values(ascending=False)

with st.sidebar:
    st.divider()
    st.header("í•„í„°")
    q = st.text_input("ê´€ì¸¡ì†Œ ê²€ìƒ‰(ë¶€ë¶„ì¼ì¹˜)", value="")
    show_all_sites = st.checkbox("ì „ì²´ ê´€ì¸¡ì†Œ ëª©ë¡ í‘œì‹œ(ëŠë¦´ ìˆ˜ ìžˆìŒ)", value=False)

    if q.strip():
        options = [s for s in site_counts.index.tolist() if q.lower() in str(s).lower()][:300]
    else:
        options = site_counts.index.tolist() if show_all_sites else site_counts.index[:200].tolist()

    if not options:
        st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    selected_site = st.selectbox("ê´€ì¸¡ì†Œ(site)", options=options, index=0)

    df_site_all = df[df["site"] == selected_site].copy()
    min_d = df_site_all["date"].min().date()
    max_d = df_site_all["date"].max().date()

    date_range = st.date_input("ê¸°ê°„", value=(min_d, max_d), min_value=min_d, max_value=max_d)
    if isinstance(date_range, tuple) and len(date_range) == 2:
        start_date, end_date = date_range
    else:
        start_date, end_date = min_d, max_d

    use_imputed = st.checkbox("ë³´ì •ê°’(is_imputed=1) í¬í•¨", value=True)

    # ì˜ˆì¸¡/ì•ŒëžŒì—ì„œ ì‚¬ìš©
    horizon = st.slider("ì˜ˆì¸¡ê¸°ê°„(ì¼)", min_value=7, max_value=30, value=14, step=1)
    target = st.selectbox(
        "ì˜ˆì¸¡/ê°ì‹œ ì§€í‘œ",
        options=[
            ("overall_aqi", "ì¢…í•© AQI(ìµœëŒ€ê°’ ê¸°ì¤€)"),
            ("o3_aqi", "O3 AQI"),
            ("no2_aqi", "NO2 AQI"),
            ("co_aqi", "CO AQI"),
            ("so2_aqi", "SO2 AQI"),
        ],
        format_func=lambda x: x[1],
    )[0]

# ê¸°ê°„/ë³´ì • í¬í•¨ ë°˜ì˜
mask = (df_site_all["date"].dt.date >= start_date) & (df_site_all["date"].dt.date <= end_date)
df_site = df_site_all.loc[mask].copy()
if not use_imputed:
    df_site = df_site[df_site["is_imputed"] == 0].copy()
df_site = df_site.sort_values("date").reset_index(drop=True)

df_model = df_site_all.copy()
if not use_imputed:
    df_model = df_model[df_model["is_imputed"] == 0].copy()
df_model = df_model.sort_values("date").reset_index(drop=True)

if len(df_model) == 0:
    st.warning("í•„í„° ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# ìµœì‹  ìœ íš¨ í–‰(ì¢…í•©AQIê°€ NaNìœ¼ë¡œ ë³´ì´ëŠ” ë¬¸ì œ ë³´ê°•)
latest_row = pick_latest_valid_row(df_model, prefer_cols=["overall_aqi"] + AQI_COLS)
latest_overall = latest_row.get("overall_aqi", np.nan)
if pd.isna(latest_overall):
    latest_overall = compute_overall_aqi_row(latest_row)


# =========================================================
# ê³µí†µ KPI(ìƒë‹¨)
# =========================================================
site_city = df_site_all["city"].mode().iloc[0] if df_site_all["city"].notna().any() else ""
site_county = df_site_all["county"].mode().iloc[0] if df_site_all["county"].notna().any() else ""
site_state = df_site_all["state"].mode().iloc[0] if df_site_all["state"].notna().any() else ""
lat, lon = latest_row.get("lat", None), latest_row.get("lon", None)

kpi = st.columns([2.2, 2.2, 2.0, 2.0, 3.6])
kpi[0].metric("ê´€ì¸¡ì†Œ", selected_site)
kpi[1].metric("ì§€ì—­", f"{site_city}, {site_county}, {site_state}")
kpi[2].metric("ê¸°ì¤€ì¼", str(pd.to_datetime(latest_row["date"]).date()))
kpi[3].metric("ì¢…í•© AQI", f"{latest_overall:.0f}" if pd.notna(latest_overall) else "N/A")
kpi[4].metric("ìƒíƒœ", f"{aqi_category(latest_overall)} / ì£¼ì˜¤ì—¼: {latest_row.get('main_pollutant','N/A')}")


# =========================================================
# íŽ˜ì´ì§€ 1) í˜„í™©
# =========================================================
if page == "í˜„í™©":
    st.subheader("í˜„í™©")

    left, right = st.columns([2.2, 1.0], gap="large")
    with left:
        st.markdown("**ìµœì‹  ì§€í‘œ**")
        show = pd.DataFrame(
            {
                "ì§€í‘œ": ["O3 AQI", "NO2 AQI", "CO AQI", "SO2 AQI", "ì¢…í•© AQI(ìµœëŒ€ê°’)"],
                "ê°’": [
                    latest_row.get("o3_aqi", np.nan),
                    latest_row.get("no2_aqi", np.nan),
                    latest_row.get("co_aqi", np.nan),
                    latest_row.get("so2_aqi", np.nan),
                    latest_overall,
                ],
                "ë¶„ë¥˜": [
                    aqi_category(latest_row.get("o3_aqi", np.nan)),
                    aqi_category(latest_row.get("no2_aqi", np.nan)),
                    aqi_category(latest_row.get("co_aqi", np.nan)),
                    aqi_category(latest_row.get("so2_aqi", np.nan)),
                    aqi_category(latest_overall),
                ],
            }
        )
        st.dataframe(show, use_container_width=True, hide_index=True)

        st.markdown("**ë°ì´í„° í’ˆì§ˆ(ì„ íƒê¸°ê°„)**")
        imputed_ratio = float(df_site["is_imputed"].mean()) if len(df_site) else 0.0
        observed_ratio = float(df_site["is_observed"].mean()) if len(df_site) else 0.0
        c1, c2, c3 = st.columns(3)
        c1.metric("ë ˆì½”ë“œ ìˆ˜", f"{len(df_site):,}")
        c2.metric("ê´€ì¸¡ ë¹„ìœ¨(is_observed=1)", f"{observed_ratio*100:.1f}%")
        c3.metric("ë³´ì • ë¹„ìœ¨(is_imputed=1)", f"{imputed_ratio*100:.1f}%")

    with right:
        st.markdown("**ê´€ì¸¡ì†Œ ìœ„ì¹˜**")
        if pd.notna(lat) and pd.notna(lon):
            st.map(pd.DataFrame({"lat": [lat], "lon": [lon]}), zoom=10)
            st.caption(f"ì¢Œí‘œ: {lat:.5f}, {lon:.5f}")
        else:
            st.info("geometry ì¢Œí‘œ ì •ë³´ê°€ ì—†ì–´ ì§€ë„ í‘œì‹œê°€ ë¶ˆê°€í•©ë‹ˆë‹¤.")

    st.divider()
    st.subheader("ì¶”ì„¸(ì‹œê³„ì—´)")

    with st.expander("í‘œì‹œ ì˜µì…˜", expanded=False):
        last_n = st.slider("ìµœê·¼ Nì¼(ì‹œê³„ì—´ í‘œì‹œ)", min_value=30, max_value=365, value=120, step=10)
        show_means = st.checkbox("Mean(í‰ê·  ë†ë„)ë„ í‘œì‹œ", value=False)

    if len(df_site) == 0:
        st.warning("ì„ íƒ ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    df_ts = df_site.sort_values("date").tail(last_n).copy()
    render_multi_line(df_ts, "date", ["overall_aqi"] + AQI_COLS, "AQI ì‹œê³„ì—´", "AQI", height=420)

    if show_means:
        render_multi_line(df_ts, "date", MEAN_COLS, "Mean(ë†ë„) ì‹œê³„ì—´", "Mean", height=360)

    st.subheader("ì›”ë³„ ì¶”ì„¸(í‰ê· )")
    df_m = df_site[["date", "overall_aqi"] + AQI_COLS].copy()
    df_m["month"] = df_m["date"].dt.to_period("M").dt.to_timestamp()
    df_m_agg = df_m.groupby("month")[["overall_aqi"] + AQI_COLS].mean().reset_index()
    render_multi_line(df_m_agg, "month", ["overall_aqi"] + AQI_COLS, "ì›”ë³„ í‰ê·  AQI", "AQI", height=360)

    st.divider()
    st.subheader("ë‹¤ìš´ë¡œë“œ")
    out_df = df_site.copy()
    csv_bytes = out_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        label="ì„ íƒê¸°ê°„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ(CSV)",
        data=csv_bytes,
        file_name=f"air_quality_{selected_site[:40].replace(' ', '_')}_range.csv",
        mime="text/csv",
    )
    st.caption("â€» ì¢…í•© AQI(overall_aqi)ëŠ” O3/NO2/CO/SO2 AQI ì¤‘ ìµœëŒ€ê°’(ë³´ìˆ˜ì  ìš´ì˜)ìž…ë‹ˆë‹¤.")


# =========================================================
# íŽ˜ì´ì§€ 2) ì˜ˆì¸¡
# =========================================================
elif page == "ì˜ˆì¸¡":
    st.subheader("ì˜ˆì¸¡")

    st.caption("ì˜ˆì¸¡ ì—”ì§„: scikit-learn" if SKLEARN_OK else "ì˜ˆì¸¡ ì—”ì§„: numpy Ridge(ì„ í˜•) (sklearn ë¯¸ì„¤ì¹˜ ëŒ€ì²´)")

    with st.spinner("ì˜ˆì¸¡ ë°ì´í„° ìƒì„± ì¤‘..."):
        fr = train_and_forecast_site(df_model, target, horizon=int(horizon), lags=14)

    if fr.mae is None or fr.pred_df.empty:
        st.warning("í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì˜ˆì¸¡ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ê¸°ê°„/ê´€ì¸¡ì†Œ ë³€ê²½ ë˜ëŠ” ë°ì´í„° ëˆ„ì  í•„ìš”)")
        st.stop()

    st.metric("ë°±í…ŒìŠ¤íŠ¸ MAE", f"{fr.mae:.2f}")
    pred_df = fr.pred_df.copy()

    # anomaly
    clim = make_climatology(df_model, target)
    pred_df["doy"] = pred_df["date"].dt.dayofyear
    pred_df["climatology"] = pred_df["doy"].map(clim).astype(float)
    pred_df["anomaly"] = pred_df["pred"] - pred_df["climatology"]

    hist = df_model[["date", target]].dropna().sort_values("date").tail(120).rename(columns={target: "actual"})
    merged = hist.merge(pred_df[["date", "pred", "climatology", "anomaly"]], on="date", how="outer").sort_values("date")

    render_multi_line(
        merged,
        "date",
        [c for c in ["actual", "pred", "climatology"] if c in merged.columns],
        "ì‹¤ì¸¡ vs ì˜ˆì¸¡ vs í´ë¼ì´ë§ˆí†¨ë¡œì§€",
        target,
        height=420,
    )
    render_single_line(pred_df, "date", "anomaly", "ì˜ˆì¸¡ anomaly(ì˜ˆì¸¡-í‰ë…„)", "anomaly", height=240)

    st.divider()
    st.subheader("ì˜ˆì¸¡ ë°ì´í„°")
    st.dataframe(pred_df[["date", "pred", "climatology", "anomaly"]], use_container_width=True, hide_index=True)

    csv_bytes = pred_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        label="ì˜ˆì¸¡ ë°ì´í„° ë‹¤ìš´ë¡œë“œ(CSV)",
        data=csv_bytes,
        file_name=f"forecast_{selected_site[:40].replace(' ', '_')}_{target}.csv",
        mime="text/csv",
    )


# =========================================================
# íŽ˜ì´ì§€ 3) ì•ŒëžŒ
# =========================================================
else:
    st.subheader("ì•ŒëžŒ")

    with st.sidebar:
        st.divider()
        st.header("ì•ŒëžŒ ê¸°ì¤€(ì´ìƒì§•í›„)")
        alert_threshold = st.number_input("ìž„ê³„ê°’(AQI)", min_value=0.0, max_value=500.0, value=101.0, step=1.0)
        sustain_days = st.number_input("ì§€ì†ì¼ìˆ˜(ì—°ì†)", min_value=1, max_value=14, value=2, step=1)
        anom_threshold = st.number_input("anomaly ìž„ê³„(+)", min_value=0.0, max_value=500.0, value=25.0, step=1.0)
        delta_threshold = st.number_input("ì „ì¼ ëŒ€ë¹„ ê¸‰ë“±(Î”) ìž„ê³„(+)", min_value=0.0, max_value=500.0, value=30.0, step=1.0)
        earlywarn_days = st.number_input("ì¡°ê¸°ê²½ë³´(ì˜ˆì¸¡) ìœˆë„ìš°(ì¼)", min_value=0, max_value=30, value=7, step=1)

        st.header("ì•Œë¦¼ ì±„ë„(ì„ íƒ)")
        enable_slack = st.checkbox("Slack Webhook ì•Œë¦¼ ì‚¬ìš©", value=False)
        slack_webhook = st.text_input("Slack Webhook URL", type="password", value=get_secret_safe("SLACK_WEBHOOK_URL", ""))
        notify_watch = st.checkbox("WATCH(ì£¼ì˜)ë„ ì™¸ë¶€ ì „ì†¡", value=False)

    # ì•ŒëžŒì—ì„œë§Œ ì˜ˆì¸¡ì„ ì‚¬ìš©(ì¡°ê¸°ê²½ë³´ìš©)
    pred_df = pd.DataFrame(columns=["date", "pred"])
    with st.expander("ì¡°ê¸°ê²½ë³´ë¥¼ ìœ„í•´ ì˜ˆì¸¡ ìƒì„±(ê¶Œìž¥)", expanded=True):
        with st.spinner("ì˜ˆì¸¡ ë°ì´í„° ìƒì„± ì¤‘..."):
            fr = train_and_forecast_site(df_model, target, horizon=int(horizon), lags=14)
        if fr.mae is None or fr.pred_df.empty:
            st.info("ì˜ˆì¸¡ ìƒì„± ë¶ˆê°€ â†’ ì‹¤ì¸¡ ê¸°ë°˜ ì•ŒëžŒë§Œ ì ìš©ë©ë‹ˆë‹¤.")
        else:
            st.caption(f"ë°±í…ŒìŠ¤íŠ¸ MAE: {fr.mae:.2f}")
            pred_df = fr.pred_df.copy()
            st.dataframe(pred_df, use_container_width=True, hide_index=True)

    alert_state = evaluate_alert_state(
        df_site=df_model,
        target_col=target,
        alert_threshold=float(alert_threshold),
        sustain_days=int(sustain_days),
        anom_threshold=float(anom_threshold),
        delta_threshold=float(delta_threshold),
        earlywarn_days=int(earlywarn_days),
        forecast_df=pred_df if not pred_df.empty else None,
    )

    level = alert_state["level"]
    reasons = alert_state["reasons"]

    msg = f"[{selected_site}] {target} ìƒíƒœ: {level}"
    if reasons:
        msg += " / ì‚¬ìœ : " + ", ".join(reasons)

    st.markdown("### ì•ŒëžŒ ìƒíƒœ")
    if level == "ALERT":
        st.error(msg)
        toast("ê²½ë³´(ALERT) ë°œìƒ", icon="ðŸš¨")
    elif level == "WATCH":
        st.warning(msg)
        toast("ì£¼ì˜(WATCH) ê°ì§€", icon="âš ï¸")
    else:
        st.success(msg)

    # ì„¸ì…˜ ë¡œê·¸(ìƒíƒœ ë³€í™” ì´ë²¤íŠ¸)
    if "alert_events" not in st.session_state:
        st.session_state["alert_events"] = []
    prev_level = st.session_state.get("prev_alert_level")

    if prev_level != level:
        st.session_state["prev_alert_level"] = level
        st.session_state["alert_events"].append(
            {
                "ts": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
                "site": selected_site,
                "target": target,
                "level": level,
                "reasons": "; ".join(reasons) if reasons else "",
            }
        )

        # ì™¸ë¶€ ì „ì†¡(ìƒíƒœ ë³€í™” ì‹œ 1íšŒ)
        if enable_slack and slack_webhook:
            if level == "ALERT" or (notify_watch and level == "WATCH"):
                ok = send_slack_webhook(slack_webhook, msg)
                st.caption("Slack ì „ì†¡: " + ("ì„±ê³µ" if ok else "ì‹¤íŒ¨(ì›¹í›…/ë„¤íŠ¸ì›Œí¬ í™•ì¸)"))

    st.divider()
    st.markdown("### ì•Œë¦¼ ë¡œê·¸(ì„¸ì…˜)")
    events_df = pd.DataFrame(st.session_state["alert_events"])
    if len(events_df):
        st.dataframe(events_df, use_container_width=True, hide_index=True)
        c1, c2 = st.columns([1, 3])
        with c1:
            if st.button("ë¡œê·¸ ì´ˆê¸°í™”"):
                st.session_state["alert_events"] = []
                st.session_state["prev_alert_level"] = None
                st.rerun()
        with c2:
            csv_bytes = events_df.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                label="ì•Œë¦¼ ë¡œê·¸ ë‹¤ìš´ë¡œë“œ(CSV)",
                data=csv_bytes,
                file_name=f"alert_events_{selected_site[:40].replace(' ', '_')}.csv",
                mime="text/csv",
            )
    else:
        st.caption("ìƒíƒœ ë³€í™” ì´ë²¤íŠ¸ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.")

    st.divider()
    st.markdown("### ê²½ë³´ íŒì • í…Œì´ë¸”(ìµœê·¼ ì‹¤ì¸¡ + ì˜ˆì¸¡)")
    recent_actual = df_model[["date", target]].dropna().sort_values("date").tail(14).copy()
    recent_actual["kind"] = "actual"
    recent_actual = recent_actual.rename(columns={target: "value"})

    future_forecast = pred_df.copy()
    if not future_forecast.empty:
        future_forecast["kind"] = "forecast"
        future_forecast = future_forecast.rename(columns={"pred": "value"})
    else:
        future_forecast = pd.DataFrame(columns=["date", "kind", "value"])

    log = pd.concat([recent_actual[["date", "kind", "value"]], future_forecast[["date", "kind", "value"]]], axis=0)
    log = log.sort_values("date").reset_index(drop=True)

    log["aqi_cat"] = log["value"].apply(aqi_category)
    log["sustain_count"] = sustained_flags(log["value"], float(alert_threshold)).astype(int)
    log["alert"] = np.where(log["sustain_count"] >= int(sustain_days), "ON", "OFF")

    st.dataframe(log.assign(date=log["date"].dt.date), use_container_width=True, hide_index=True)

            delta_color=delta_color
        )
